{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyopenms import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "# suppress CUDA logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], \n",
    "                                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from ionmob.alignment.experiment import Experiment\n",
    "\n",
    "from ionmob.models.deep_models import ProjectToInitialSqrtCCS, DeepRecurrentModel, DeepRecurrentConvModel\n",
    "from ionmob.preprocess.data import get_tf_dataset, partition_tf_dataset, to_tf_dataset\n",
    "from ionmob.preprocess.helpers import get_sqrt_slopes_and_intercepts, sequence_to_tokens, fit_tokenizer\n",
    "from ionmob.preprocess.helpers import tokenizer_to_json, tokenizer_from_json, split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_phospho(seq):\n",
    "    for char in seq:\n",
    "        if char.find('<PH>') != -1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenizer_from_json('../pretrained-models/tokenizers/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN data\n",
    "# Meier et al.\n",
    "meier = pd.read_parquet('../data/Meier.parquet')\n",
    "\n",
    "# Tenzer data\n",
    "zepeda = pd.read_parquet('../data/Zepeda_unique.parquet')\n",
    "tenzer_p = pd.read_parquet('../data/Tenzer-phospho-train.parquet')\n",
    "tenzer_p['is_phos'] = tenzer_p.apply(lambda r: is_phospho(['sequence-tokenized']), axis=1)\n",
    "tenzer_p = tenzer_p[tenzer_p.is_phos]\n",
    "\n",
    "# validation data\n",
    "tenzer = pd.read_parquet('../data/Tenzer_unique.parquet')\n",
    "tenzer_p_valid = pd.read_parquet('../data/Tenzer-phospho-valid_unique.parquet')\n",
    "\n",
    "# TEST data\n",
    "chang = pd.read_parquet('../data/Chang_unique.parquet')\n",
    "sara = pd.read_parquet('../data/Sara_unique.parquet')\n",
    "ogata = pd.read_parquet('../data/Ogata_unique.parquet')\n",
    "\n",
    "# shuffle and split\n",
    "TRAIN = pd.concat([meier, zepeda, tenzer_p]).sample(frac=1.0)\n",
    "\n",
    "# TRAIN, VALID, TEST = split_dataset(TRAIN)\n",
    "\n",
    "VALID = pd.concat([tenzer, tenzer_p_valid])\n",
    "\n",
    "train = to_tf_dataset(TRAIN.mz.values, TRAIN.charge.values, \n",
    "                      [list(x) for x in TRAIN['sequence-tokenized'].values], \n",
    "                      TRAIN.ccs.values, tokenizer, batch=False)\n",
    "\n",
    "train = train.shuffle(TRAIN.shape[0] + 1).batch(128).prefetch(5)\n",
    "\n",
    "validation = to_tf_dataset(VALID.mz.values, VALID.charge.values, \n",
    "                           [list(x) for x in VALID['sequence-tokenized'].values], \n",
    "                           VALID.ccs.values, tokenizer, batch=False)\n",
    "\n",
    "validation = validation.batch(2024).prefetch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19191f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "slopes, intercepts = get_sqrt_slopes_and_intercepts(TRAIN.mz.values, TRAIN.charge.values, TRAIN.ccs.values, fit_charge_state_one=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ede7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "early_stopper = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_output_1_loss',\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='training/rnn/checkpoint',\n",
    "    monitor='val_output_1_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(\n",
    "    filename='training/rnn/training.csv',\n",
    "    separator=',',\n",
    "    append=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_output_1_loss',\n",
    "    factor=1e-1,\n",
    "    patience=3,\n",
    "    monde='auto',\n",
    "    min_delta=1e-5,\n",
    "    cooldown=0,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "cbs = [early_stopper, checkpoint, csv_logger, reduce_lr, tensorboard_callback]\n",
    "\n",
    "model = DeepRecurrentModel(slopes, \n",
    "                           intercepts,\n",
    "                           gru_1=128,\n",
    "                           gru_2=128,\n",
    "                           num_tokens=len(tokenizer.word_index), \n",
    "                           do=0.2)\n",
    "\n",
    "model.build([(None, 1), (None, 4), (None, 50)])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.MeanAbsoluteError(), loss_weights=[1., 0.0],\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['mae', 'mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train, validation_data=validation, \n",
    "                    epochs=100, verbose=True, callbacks=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b6ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training and validation loss \n",
    "plt.figure(figsize=(8, 4), dpi=200)\n",
    "plt.plot(history.history['output_1_mae'], label='training')\n",
    "plt.plot(history.history['val_output_1_mae'], label='validation')\n",
    "# plt.hlines(9.5, xmin=0, xmax=len(history.history['output_1_mae']), linestyles='dashed', color='black', linewidth=1, alpha=.75)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
